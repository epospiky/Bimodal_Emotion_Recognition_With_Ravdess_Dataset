{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b86fbd7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133eb72d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy, MulticlassConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb536ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BimodalEmotionRecognitionModel(pl.LightningModule):\n",
    "    def __init__(self, input_size_audio, input_size_video, hidden_size=64, num_classes=8, vocab=None, extra_hidden_size=64):\n",
    "        super(BimodalEmotionRecognitionModel, self).__init__()\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "        # Defining the layers\n",
    "        self.audio_fc = nn.Linear(input_size_audio, hidden_size)\n",
    "        self.audio_hidden = nn.Linear(hidden_size, extra_hidden_size)\n",
    "        self.audio_hidden2 = nn.Linear(hidden_size, extra_hidden_size)        \n",
    "        \n",
    "        self.video_fc = nn.Linear(input_size_video, hidden_size)\n",
    "        self.video_hidden = nn.Linear(hidden_size, extra_hidden_size)\n",
    "        self.video_hidden2 = nn.Linear(hidden_size, extra_hidden_size)\n",
    "\n",
    "        \n",
    "        self.final_fc = nn.Linear(extra_hidden_size * 2, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.f1_score = MulticlassF1Score(num_classes=num_classes)\n",
    "        self.accuracy = MulticlassAccuracy(num_classes=num_classes)\n",
    "        self.conf_matrix = MulticlassConfusionMatrix(num_classes=num_classes)\n",
    "\n",
    "        # Store the vocabulary\n",
    "        self.vocab = vocab\n",
    "        # Calculate class weights to handle imbalance\n",
    "        class_counts = torch.tensor([96, 192, 192, 192, 192, 192, 192, 192])  # Update with actual class counts\n",
    "        total_samples = class_counts.sum().float()\n",
    "        class_weights = total_samples / (num_classes * class_counts)\n",
    "\n",
    "        # Convert class weights to a tensor and send to the device\n",
    "        self.class_weights = class_weights.to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, audio, video):\n",
    "        audio = self.relu(self.audio_fc(audio.view(audio.size(0), -1)))\n",
    "        audio = self.relu(self.audio_hidden(audio))\n",
    "        audio = self.relu(self.audio_hidden2(audio))\n",
    "\n",
    "        video = self.relu(self.video_fc(video.view(video.size(0), -1)))\n",
    "        video = self.relu(self.video_hidden(video))\n",
    "        video = self.relu(self.video_hidden2(video))\n",
    "\n",
    "        \n",
    "        combined = torch.cat([audio, video], dim=1)\n",
    "        output = self.final_fc(combined)\n",
    "\n",
    "        return output\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        audio, video, target = batch\n",
    "        output = self(audio, video)\n",
    "        \n",
    "        # Weighted loss computation\n",
    "        loss = F.cross_entropy(output, target, weight=self.class_weights)\n",
    "        f1 = self.f1_score(output.argmax(dim=1), target)\n",
    "        acc = self.accuracy(output.argmax(dim=1), target)  \n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        #log to wandb\n",
    "        wandb.log({\"train_loss\":loss, \"train_f1\":f1, \"train_acc\":acc})\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        audio, video, target = batch\n",
    "        output = self(audio, video)\n",
    "        loss = self.loss(output, target)\n",
    "        f1 = self.f1_score(output.argmax(dim=1), target)\n",
    "        acc = self.accuracy(output.argmax(dim=1), target)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        self.conf_matrix.update(output.argmax(dim=1), target)\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        #log to wandb\n",
    "        wandb.log({\"val_loss\":loss, \"f1_val\":f1, \"val_acc\":acc})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Move the confusion matrix to CPU and then convert to NumPy\n",
    "        confusion_matrix = self.conf_matrix.compute().cpu().numpy()\n",
    "\n",
    "        # Read class names from vocab.json\n",
    "        with open(\"/mnt/c/users/admin/desktop/github/bimodal_emotion_recognition_with_ravdess_dataset/vocab.json\", \"r\") as f:\n",
    "            vocab_data = json.load(f)\n",
    "        \n",
    "\n",
    "        # Extract class names in order\n",
    "        class_names = [vocab_data[\"idx2label\"][str(i)] for i in range(len(vocab_data[\"idx2label\"]))]\n",
    "\n",
    "        # Display the confusion matrix\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.show()\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        audio, video, target = batch\n",
    "        output = self(audio, video)\n",
    "        loss = self.loss(output, target)\n",
    "        f1 = self.f1_score(output.argmax(dim=1), target)\n",
    "        acc = self.accuracy(output.argmax(dim=1), target)\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        #log to wandb\n",
    "        wandb.log({\"test_loss\":loss, \"test_f1\":f1, \"test_acc\":acc})\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121cb50",
   "metadata": {},
   "source": [
    "# Datamodule/Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class BimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, vocab_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_feature = np.load(self.data['video_feature_path'][idx])\n",
    "        audio_feature = np.load(self.data['audio_feature_path'][idx])\n",
    "        label = torch.tensor(self.vocab['label2idx'][self.data['label'][idx]])\n",
    "\n",
    "        return audio_feature, video_feature, label\n",
    "\n",
    "class BimodalEmotionRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=12):\n",
    "        super(BimodalEmotionRecognitionDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BimodalEmotionDataset(\"/mnt/c/users/admin/desktop/github/bimodal_emotion_recognition_with_ravdess_dataset/example.csv\", \"/mnt/c/users/admin/desktop/github/bimodal_emotion_recognition_with_ravdess_dataset/vocab.json\")\n",
    "        train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "        train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "        self.dataset = dataset\n",
    "        self.train_dataset = train\n",
    "        self.val_dataset = val\n",
    "        self.test_dataset = test\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def whole_dataset_class_distribution(self):\n",
    "        labels = [item[2].item() for item in self.train_dataset + self.val_dataset + self.test_dataset]\n",
    "        class_counts = np.bincount(labels)\n",
    "        return class_counts\n",
    "    \n",
    "    def class_distribution(self, dataset):\n",
    "        labels = [item[2].item() for item in dataset]\n",
    "        class_counts = np.bincount(labels)\n",
    "        return class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b8fb1",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951e6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=\"Emotion_Reg_Evalu\", config={\"input_size_audio\": 512, \"input_size_video\": 1568 * 768, \"hidden_size\": 64, \"num_classes\": 8, \"extra_hidden_size\": 64})\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    data_module = BimodalEmotionRecognitionDataModule(batch_size=128, num_workers=12)\n",
    "    data_module.setup()\n",
    "    # Get class names from the vocabulary loaded during setup\n",
    "    class_names = [data_module.dataset.vocab[\"idx2label\"][str(i)] for i in range(len(data_module.dataset.vocab[\"idx2label\"]))]\n",
    "\n",
    "    # Plot class distribution for the whole dataset\n",
    "    whole_dataset_distribution = data_module.whole_dataset_class_distribution()\n",
    "    plt.bar(class_names, whole_dataset_distribution)\n",
    "    plt.title('Whole Dataset Class Distribution')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "    # Plot class distribution for the training dataset\n",
    "    train_dataset_distribution = data_module.class_distribution(data_module.train_dataset)\n",
    "    plt.bar(class_names, train_dataset_distribution)\n",
    "    plt.title('Training Dataset Class Distribution')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "    # Plot class distribution for the validation dataset\n",
    "    val_dataset_distribution = data_module.class_distribution(data_module.val_dataset)\n",
    "    plt.bar(class_names, val_dataset_distribution)\n",
    "    plt.title('Validation Dataset Class Distribution')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "    # Plot class distribution for the test dataset\n",
    "    test_dataset_distribution = data_module.class_distribution(data_module.test_dataset)\n",
    "    plt.bar(class_names, test_dataset_distribution)\n",
    "    plt.title('Test Dataset Class Distribution')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "    \n",
    "    model = BimodalEmotionRecognitionModel(\n",
    "        input_size_audio=512,\n",
    "        input_size_video=1568 * 768,\n",
    "        hidden_size=64,\n",
    "        num_classes=8,\n",
    "        extra_hidden_size=wandb.config.extra_hidden_size  \n",
    "    ).to(device)\n",
    "\n",
    "    \n",
    "    wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=50\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, data_module)\n",
    "    trainer.test(model, datamodule=data_module)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
